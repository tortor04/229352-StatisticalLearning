{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tortor04/229352-StatisticalLearning/blob/main/lab02_data_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X33DsAOeokmy"
      },
      "source": [
        "### Statistical Learning for Data Science 2 (229352)\n",
        "#### Instructor: Donlapark Ponnoprat\n",
        "\n",
        "#### [Course website](https://donlapark.pages.dev/229352/)\n",
        "\n",
        "## Lab #2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score, roc_curve\n",
        "\n",
        "# For Fashion-MNIST\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "\n",
        "# For 20 Newsgroups\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', 1000)"
      ],
      "metadata": {
        "id": "JOnoRSjo8dd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Marketing Campaign Dataset - Manual Data Preprocessing & Logistic Regression"
      ],
      "metadata": {
        "id": "exuopfRD8mHt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the Marketing Campaign Dataset ([Data Information](https://archive.ics.uci.edu/dataset/222/bank+marketing))\n",
        "\n",
        "The data is related with direct marketing campaigns of a Portuguese banking institution. The marketing campaigns were based on phone calls. Often, more than one contact to the same client was required, in order to access if the product (bank term deposit) would be (`'yes'`) or not (`'no'`) subscribed."
      ],
      "metadata": {
        "id": "FdWsJYcVBf1Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# URL for the raw Bank Marketing dataset CSV\n",
        "bank_url = 'https://raw.githubusercontent.com/donlap/ds352-labs/main/bank.csv'\n",
        "\n",
        "# Load the dataset\n",
        "# - sep=';' is crucial as it's semicolon-separated\n",
        "# - na_values=['unknown'] converts 'unknown' strings to NaN\n",
        "df = pd.read_csv(bank_url, sep=';', na_values=['unknown'])\n",
        "df = df.drop([\"emp.var.rate\", \"cons.price.idx\", \"cons.conf.idx\",\t\"euribor3m\", \"nr.employed\"], axis=1)\n",
        "print(\"Shape of the dataset:\", df.shape)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "CJ9ujPUBBcTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Exploration"
      ],
      "metadata": {
        "id": "x4invEyyBRDI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values\n",
        "print(\"--- Missing Values Count ---\")\n",
        "print(df.isnull().sum())"
      ],
      "metadata": {
        "id": "u6wANpgzBQoK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check unique values and counts for categorical columns\n",
        "print(\"--- Unique Values for Categorical Columns ---\")\n",
        "for col in df.select_dtypes(include='object').columns:\n",
        "    print(f\"\\n'{col}' unique values:\")\n",
        "    print(df[col].value_counts(dropna=False)) # Include NaN counts"
      ],
      "metadata": {
        "id": "1ack00iLnuCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preprocessing"
      ],
      "metadata": {
        "id": "iCEtO_EWDTzC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Map target variable 'y' to 0 (no) and 1 (yes)\n",
        "df['y'] = # Write your code here\n",
        "\n",
        "# Calculate class statistics\n",
        "\n",
        "\n",
        "# Drop 'duration' due to data leakage\n",
        "\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "\n",
        "\n",
        "# Split the data BEFORE any transformations\n",
        "\n",
        "\n",
        "# Print data shape\n",
        "\n"
      ],
      "metadata": {
        "id": "W2J1Prc_BXe7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will apply `StandardScaler()`, `OrdinalEncoder()`, and `OneHotEncoder()` on a few selected columns."
      ],
      "metadata": {
        "id": "yM_gRkNjGR53"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Numerical Feature: `age` and `campaign` (Standard Scaling)**"
      ],
      "metadata": {
        "id": "r4sPr5qDGY65"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the columns for demonstration\n",
        "num_cols_demo = ['age', 'campaign']\n",
        "\n",
        "# Initialize StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler ONLY on the training data\n",
        "## Write your code here\n",
        "\n",
        "# Transform both training and test data\n",
        "X_train_scaled_demo = # Write your code here\n",
        "X_test_scaled_demo = # Write your code here"
      ],
      "metadata": {
        "id": "52ibTHHFDxSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at the transformed `age` and `campaign` features and their statistics."
      ],
      "metadata": {
        "id": "YnT6pEq4LlZg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nOriginal X_train 'age' and 'campaign' head:\")\n",
        "print(X_train[num_cols_demo].head())\n",
        "print(\"\\nScaled X_train 'age' and 'campaign' head:\")\n",
        "print(pd.DataFrame(X_train_scaled_demo, columns=num_cols_demo, index=X_train.index).head())\n",
        "\n",
        "print(\"\\nMean of scaled 'age' (train):\", X_train_scaled_demo[:, 0].mean())\n",
        "print(\"Std Dev of scaled 'campaign' (train):\", X_train_scaled_demo[:, 1].std())"
      ],
      "metadata": {
        "id": "7bfFm4InLiGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Ordinal Feature: `education` (Ordinal Encoding with Imputation)**"
      ],
      "metadata": {
        "id": "8jQi5cJAGj_m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Imputation**"
      ],
      "metadata": {
        "id": "2uyk1rA7K7_p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the column for demonstration\n",
        "ord_col_demo = ['education']\n",
        "\n",
        "# Step 1: Impute missing values using SimpleImputer (most frequent strategy)\n",
        "imputer_ord = SimpleImputer(strategy='most_frequent')\n",
        "\n",
        "# Fit imputer only on training data\n",
        "## Write your code here\n",
        "\n",
        "# Fit imputer only on training AND test data\n",
        "X_train_imputed_ord_demo = # Write your code here\n",
        "X_test_imputed_ord_demo = # Write your code here"
      ],
      "metadata": {
        "id": "A8RbFYHLIhju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Ordinal Encoding**"
      ],
      "metadata": {
        "id": "P9hDuDPyLGaA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the order for the 'education' ordinal feature\n",
        "education_categories = [\n",
        "    'illiterate', 'basic.4y', 'basic.6y', 'basic.9y', 'high.school',\n",
        "    'professional.course', 'university.degree', 'masters', 'doctorate'\n",
        "]"
      ],
      "metadata": {
        "id": "n7DMoQ6AGf8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Initialize OrdinalEncoder with the predefined categories\n",
        "ordinal_encoder = OrdinalEncoder(categories=[education_categories])\n",
        "\n",
        "# Fit the encoder ONLY on the imputed training data\n",
        "## Write your code here\n",
        "\n",
        "# Transform both imputed training and test data\n",
        "X_train_ord_encoded_demo = # Write your code here\n",
        "X_test_ord_encoded_demo = # Write your code here"
      ],
      "metadata": {
        "id": "MjbM7wi8IfHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at the imputed and ordinal-encoded `education`."
      ],
      "metadata": {
        "id": "tmYa8EfcOCcv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nOriginal X_train 'education' head:\")\n",
        "print(X_train[ord_col_demo].iloc[20:25])\n",
        "print(\"\\nImputed X_train 'education' head (after imputer.transform):\")\n",
        "print(pd.DataFrame(X_train_imputed_ord_demo, columns=ord_col_demo, index=X_train.index).iloc[20:25])\n",
        "print(\"\\nOrdinal Encoded X_train 'education' head:\")\n",
        "print(pd.DataFrame(X_train_ord_encoded_demo, columns=ord_col_demo, index=X_train.index).iloc[20:25])"
      ],
      "metadata": {
        "id": "JrJoFy3-Ik1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Nominal Feature: `job` (One-Hot Encoding with Imputation)**"
      ],
      "metadata": {
        "id": "GarEnu8iK1lX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Imputation**"
      ],
      "metadata": {
        "id": "1eJSiEslLNi4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the column for demonstration\n",
        "nom_col_demo = ['job']\n",
        "\n",
        "# Step 1: Impute missing values using SimpleImputer (most frequent strategy)\n",
        "imputer_nom = SimpleImputer(strategy='most_frequent')\n",
        "imputer_nom.fit(X_train[nom_col_demo]) # Fit imputer only on training data\n",
        "\n",
        "X_train_imputed_nom_demo = imputer_nom.transform(X_train[nom_col_demo])\n",
        "X_test_imputed_nom_demo = imputer_nom.transform(X_test[nom_col_demo])"
      ],
      "metadata": {
        "id": "1OUbef1bJPUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Nominal Encoding**"
      ],
      "metadata": {
        "id": "l7F_kim6TQ72"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sparse_output=False ensures a dense NumPy array is returned, easier for viewing\n",
        "onehot_encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
        "\n",
        "# Fit the encoder ONLY on the imputed training data\n",
        "## Write your code here\n",
        "\n",
        "# Transform both imputed training and test data\n",
        "X_train_onehot_encoded_demo = ## Write your code here\n",
        "X_test_onehot_encoded_demo = ## Write your code here"
      ],
      "metadata": {
        "id": "6kMCGjksLyrf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nOriginal X_train 'job' head:\")\n",
        "print(X_train[nom_col_demo].iloc[40:45])\n",
        "print(\"\\nImputed X_train 'job' head (after imputer.transform):\")\n",
        "print(pd.DataFrame(X_train_imputed_nom_demo, columns=nom_col_demo, index=X_train.index).iloc[40:45])\n",
        "print(\"\\nOne-Hot Encoded X_train 'job' shape:\", X_train_onehot_encoded_demo.shape)\n",
        "print(\"First 5 rows of One-Hot Encoded X_train 'job':\")\n",
        "print(pd.DataFrame(X_train_onehot_encoded_demo, columns=onehot_encoder.get_feature_names_out(nom_col_demo), index=X_train.index).iloc[40:45])"
      ],
      "metadata": {
        "id": "j1qIKIelM0yX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 1: Apply All Preprocessing & Train Logistic Regression**\n",
        "\n",
        "Now, it's your turn to apply these preprocessing steps to *all* relevant columns and then train a Logistic Regression model.\n",
        "\n",
        "**Instructions:**\n",
        "\n",
        "1.  Look at the Variable Table in [this link](https://archive.ics.uci.edu/dataset/222/bank+marketing).\n",
        "2. Make lists for `numerical_features`, `ordinal_features`, and `nominal_features`.\n",
        "3. Preprocess the features. It is safer to make a copy of `X_train` using:\n",
        "   ```\n",
        "   X_train_copy = X_train.copy()\n",
        "   X_test_copy = X_test.copy()\n",
        "   ```\n",
        "   and preprocess `X_train_copy` instead.\n",
        "\n",
        "   **For nominal features, concat the one-hot encoded features using [`pd.concat(..., axis=1)`](https://pandas.pydata.org/docs/reference/api/pandas.concat.html) and drop the old nominal features from the dataframe.**\n",
        "4. Train Logistic Regression on the preprocessed `X_train_copy` and `y_train`.\n",
        "5. Evaluate the Model:\n",
        "    *   Make predictions on the preprocessed `X_test_copy`.\n",
        "    *   Print `classification_report` ([Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html)). What are the accuracy, average precision, average recall, and average f1-score?\n"
      ],
      "metadata": {
        "id": "CllpFvNBNYWI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- YOUR CODE FOR EXERCISE 1 STARTS HERE ---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xuI8hAlIRfDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: Fashion-MNIST Dataset - Image Classification"
      ],
      "metadata": {
        "id": "m9qrm2DKRtgm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Fashion-MNIST Dataset\n",
        "\n",
        "The Fashion-MNIST dataset consists of 28x28 grayscale images of fashion items."
      ],
      "metadata": {
        "id": "kc8SZBvcS8_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "(fm_X_train, fm_y_train), (fm_X_test, fm_y_test) = fashion_mnist.load_data()\n",
        "\n",
        "print(f\"Fashion-MNIST Train data shape: {fm_X_train.shape}\")\n",
        "print(f\"Fashion-MNIST Train labels shape: {fm_y_train.shape}\")\n",
        "print(f\"Fashion-MNIST Test data shape: {fm_X_test.shape}\")\n",
        "print(f\"Fashion-MNIST Test labels shape: {fm_y_test.shape}\")"
      ],
      "metadata": {
        "id": "r0FQt8rlRgoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"First image {fm_X_train[0]}\")\n",
        "print(f\"First label {fm_y_train[0]}\")"
      ],
      "metadata": {
        "id": "LXJ9EmcIVVrh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualize Fashion-MNIST Images\n",
        "\n",
        "Let's see what these images look like."
      ],
      "metadata": {
        "id": "WwkQOE79TV70"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define class names (for better understanding)\n",
        "fashion_mnist_class_names = [\n",
        "    'T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "    'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'\n",
        "]\n",
        "\n",
        "# Visualize the images\n",
        "## Write your code here"
      ],
      "metadata": {
        "id": "-YUI6IzbTGYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 2: Preprocessing Images (Flatten and Scale)**\n",
        "\n",
        "Images are 2D arrays (matrices of pixels) and pixel values are integers from 0-255. For Logistic Regression, we need:\n",
        "*  **Flattening:** Convert each 28x28 image into a 1D array of 784 features.\n",
        "*  **Scaling:** Normalize pixel values from [0, 255] to [0, 1].\n",
        "\n",
        "**Instructions:**\n",
        "\n",
        "1.   **Flatten:** Use the `.reshape()` method (see [documentation](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.reshape.html)). For `fm_X_train_binary` (shape `(num_samples, 28, 28)`), you want to reshape it to `(num_samples, 28*28)`.\n",
        "2.  **Scale:** Divide the flattened pixel values by 255.0 to get values between 0 and 1.\n",
        "3.   **Train Logistic Regression:**\n",
        "    *   Initialize `LogisticRegression(solver='saga')`. `saga` is a good solver when both number of samples and number of features are large.\n",
        "    *   Fit the model on your *processed* `fm_X_train_scaled` and `fm_y_train`.\n",
        "4.   **Make Predictions:** Use `predict()` to make predictions on the *processed* `fm_X_test_scaled`.\n",
        "5.   **Print Classification Report:** Print `classification_report` ([Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html)). What are the accuracy, average precision, average recall, and average f1-score?\n",
        "6.   **Visualize Misclassifications:**\n",
        "    *   Find the indices in `fm_X_test_binary` where your model made incorrect predictions (i.e., `fm_y_pred != fm_y_test`).\n",
        "    *   Select 5 of these misclassified images.\n",
        "    *   Plot these images (using `plt.imshow`). For each image, print its true label and its predicted label."
      ],
      "metadata": {
        "id": "cXvJB42xVrHu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- YOUR CODE FOR EXERCISE 2 STARTS HERE ---\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "P_Z7ooAgdLVf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: 20 Newsgroups Dataset - Text Classification"
      ],
      "metadata": {
        "id": "u7-q5FmnboVJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load 20 Newsgroups Dataset\n",
        "\n",
        "The 20 newsgroups dataset comprises around 18000 newsgroups posts on 20 topics."
      ],
      "metadata": {
        "id": "6TDncAyyb6mJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the training and testing data\n",
        "news_train = fetch_20newsgroups(subset='train', shuffle=True, random_state=42)\n",
        "news_test = fetch_20newsgroups(subset='test', shuffle=True, random_state=42)\n",
        "\n",
        "X_train_news, y_train_news = news_train.data, news_train.target\n",
        "X_test_news, y_test_news = news_test.data, news_test.target\n",
        "\n",
        "print(f\"Number of training documents: {len(X_train_news)}\")\n",
        "print(f\"Number of test documents: {len(X_test_news)}\")\n",
        "print(f\"Categories: {news_train.target_names}\")"
      ],
      "metadata": {
        "id": "E91xZl5NbnpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explore Sample Document"
      ],
      "metadata": {
        "id": "tVCq09V2cfGj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the first document and its class\n",
        "## Write your code here\n"
      ],
      "metadata": {
        "id": "G37RNZGebFZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing: Text Vectorization Demonstration with `TfidfVectorizer`"
      ],
      "metadata": {
        "id": "1vXXUdp7chsX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\n",
        "\\text{TF-IDF}(t, d, D) = \\text{TF}(t, d) \\times \\text{IDF}(t, D)\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "$$\n",
        "\\text{TF}(t, d) = \\frac{\\text{number of word }t\\text{ in } d}{\\text{number of words in } d} \\quad \\text{ and } \\quad\n",
        "\\text{IDF}(t, D) = \\log\\left(\\frac{\\text{total number of documents}}{\\text{number of documents that contain word }t}\\right).\n",
        "$$"
      ],
      "metadata": {
        "id": "rwaq1igbi-Hp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a small list of sentences for demonstration\n",
        "sample_sentences = [\n",
        "    \"This is the first document.\",\n",
        "    \"This document is the second document.\",\n",
        "    \"And this is the third one.\",\n",
        "    \"Is this the first document?\"\n",
        "]\n",
        "\n",
        "# Initialize TfidfVectorizer for demonstration\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "\n",
        "# Fit and transform the sample sentences\n",
        "# The output `sample_vec_output` will be a sparse matrix\n",
        "sample_vec_output_sparse = # Write your code here\n",
        "\n",
        "# Convert to a dense NumPy array for easier viewing\n",
        "sample_vec_output_dense = sample_vec_output_sparse.toarray()\n",
        "\n",
        "print(\"\\nVocabulary (word to index mapping):\")\n",
        "print(vectorizer.vocabulary_)\n",
        "\n",
        "print(\"\\nFeature names (words):\")\n",
        "print(vectorizer.get_feature_names_out())\n",
        "\n",
        "print(\"\\nTF-IDF matrix for sample sentences (dense format):\")\n",
        "print(sample_vec_output_dense)"
      ],
      "metadata": {
        "id": "8AaVd2D9ckKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Exercise 3: Apply TF-IDF Vectorization to Full Dataset**\n",
        "\n",
        "Now, apply `TfidfVectorizer` to the actual training and testing datasets for the 20 Newsgroups classification task.\n",
        "\n",
        "**Instructions:**\n",
        "\n",
        "1.  **Initialize `TfidfVectorizer`:**\n",
        "    *   Initialize `TfidfVectorizer`. Use `stop_words='english'` to remove common words.\n",
        "2.  **Fit and Transform Training Data:**\n",
        "    *   Call `fit_transform()` on `X_train_news` to learn the vocabulary and transform the training text into TF-IDF features. Store the result in `X_train_vec`.\n",
        "3.  **Transform Test Data:**\n",
        "    *   Call `transform()` on `X_test_news` using the *already fitted* vectorizer. Store the result in `X_test_vec`. **Crucially, do not call `fit_transform()` on the test data!** This would cause data leakage.\n",
        "4.  **Initialize Logistic Regression:**\n",
        "    *   Initialize `LogisticRegression(solver='saga')`. `saga` is a good solver when both number of samples and number of features are large.\n",
        "5.  **Train the Model:**\n",
        "    *   Fit the model on your `X_train_vec` and `y_train_news`.\n",
        "6.  **Make Predictions:**\n",
        "    *   Make predictions using `predict()` on the `X_test_vec`.\n",
        "7.  **Evaluate the Model:**\n",
        "    *   Print `classification_report` ([Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html)). What are the accuracy, average precision, average recall, and average f1-score?"
      ],
      "metadata": {
        "id": "aCa_dcEDc-PQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- YOUR CODE FOR EXERCISE 3 STARTS HERE ---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "B6g7y5yXrczq"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}